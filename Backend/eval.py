# import os
# import pandas as pd
# from sentence_transformers import SentenceTransformer
# from langchain_nvidia_ai_endpoints import ChatNVIDIA
# from ragas import evaluate
# from ragas.metrics import (
#     faithfulness,
#     answer_relevancy,
#     context_recall,
#     context_precision,
# )
# from rag import (
#     retrieve_context_from_chromadb,
#     generate_answer,
#     format_docs,
# )
# from datasets import Dataset
# from rag import llm

# # Instantiate the NVIDIA LLM
# # model_id = "meta/llama-3.1-8b-instruct"
# # llm = ChatNVIDIA(model=model_id, temperature=0)

# # Helper class to wrap the SentenceTransformer
# class EmbedderWrapper:
#     def __init__(self, embedder: SentenceTransformer):
#         self.embedder = embedder

#     def embed_query(self, query: str):
#         # This method generates embeddings for a single query
#         return self.embedder.encode(query, convert_to_tensor=False).tolist()

#     def embed_documents(self, documents: list):
#         # This method generates embeddings for a list of documents
#         return [self.embedder.encode(doc, convert_to_tensor=False).tolist() for doc in documents]
    
#     def embed_text(self, text: str):
#         return self.embedder.encode(text, convert_to_tensor=False).tolist()

# # Initialize the embedder and wrap it
# embedder = SentenceTransformer("all-MiniLM-L6-v2")
# wrapped_embedder = EmbedderWrapper(embedder)

# # Helper function for debugging
# def debug_response(question, context, response):
#     print("\n--- Debugging Response ---")
#     print(f"Question: {question}")
#     print(f"Context: {context[:100]}")
#     print(f"Generated Response: {response}\n")
#     print("--------------------------")

# # Load the uploaded CSV file
# file_path = "/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Filtered test cases/shipping_subscription.xlsx"
# test_data = pd.read_excel(file_path)

# # Ensure the columns are correctly loaded
# print("DataFrame columns:", test_data.columns.tolist())

# # Collect data for evaluation
# documents = []

# for index, data in test_data.iterrows():
#     question = data["question"]
#     reference = data["expected_answers"]
    
#     # Retrieve context
#     context_docs = retrieve_context_from_chromadb(question)
#     formatted_context = format_docs(context_docs)
    
#     # Generate answer using NVIDIA LLM
#     try:
#         response = generate_answer(question)
#         if not response:  # If no response is generated
#             response = "No response generated by the LLM."
#     except Exception as e:
#         response = f"Error in response generation: {e}"
    
#     # Debugging response
#     debug_response(question, formatted_context, response)
    
#     # Collect the document
#     document = {
#         "question": question,
#         "reference": reference,
#         "retrieved_contexts": [formatted_context],
#         "response": response,
#         "question_embedding": wrapped_embedder.embed_query(question),
#         "reference_embedding": wrapped_embedder.embed_query(reference),
#     }
#     documents.append(document)

# # Create a Hugging Face Dataset from the documents
# dataset = Dataset.from_list(documents)

# # Ensure the LLM is passed correctly and embeddings are used for evaluation
# result = evaluate(
#     dataset=dataset,
#     metrics=[
#         context_precision,
#         context_recall,
#         faithfulness,
#         answer_relevancy,
#     ],
#     llm=llm, 
#     embeddings=wrapped_embedder,  # Use wrapped embedder for embedding operations
# )


# df = result.to_pandas()

# print(df)

# output_file = "/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Results/shipping_subscription_eval.csv"
# df.to_csv(output_file, index=True)

# print(f"Evaluation results saved to {output_file}")

# import pandas as pd
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from bert_score import score
# import chromadb
# import os
# from dotenv import load_dotenv

# # Path to your persistent storage for ChromaDB
# load_dotenv('/Users/shoumikdaterao/Desktop/Sem\\ 7/Project/code/code/.env.local')
# storage_path = os.getenv('STORAGE_PATH')
# if storage_path is None:
#     raise ValueError('STORAGE_PATH environment variable is not set')

# # Initialize ChromaDB client and collection
# client = chromadb.PersistentClient(path=storage_path)
# collection = client.get_collection(name="customer_support_chatbot_data")

# # Load the tokenizer and model
# tokenizer = AutoTokenizer.from_pretrained("khalednabawi11/fine_tuned_dialo-gpt")
# model = AutoModelForCausalLM.from_pretrained("khalednabawi11/fine_tuned_dialo-gpt")

# # Retrieve relevant context from ChromaDB
# def retrieve_context(question, num_results=3):
#     # Embed the query using a Sentence Transformer
#     from sentence_transformers import SentenceTransformer
#     embedder = SentenceTransformer('all-MiniLM-L6-v2')
#     query_embedding = embedder.encode(question, convert_to_tensor=True).cpu().numpy()
    
#     # Query the ChromaDB collection
#     results = collection.query(query_embeddings=[query_embedding], n_results=num_results)
    
#     # Format the retrieved documents
#     if "documents" in results and results["documents"]:
#         context = "\n\n".join(doc for doc in results["documents"][0])
#         return context
#     else:
#         return "No relevant context found."

# # Function to generate model responses using RAG
# def generate_response_rag(question):
#     # Retrieve relevant context
#     context = retrieve_context(question)
    
#     # Combine context and question
#     input_text = f"{context} {tokenizer.eos_token} {question}"
    
#     # Encode the input for the model
#     inputs = tokenizer.encode(input_text, return_tensors="pt")
#     outputs = model.generate(
#         inputs,
#         max_length=2000,
#         pad_token_id=tokenizer.eos_token_id,
#         num_return_sequences=1,
#         temperature=0.7,  # Adjust temperature for randomness in responses
#         top_p=0.9        # Use nucleus sampling
#     )
#     return tokenizer.decode(outputs[0], skip_special_tokens=True)

# # Load the CSV file (ensure it has columns: 'Question' and 'Expected Answer')
# data = pd.read_csv(r"/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Filtered test cases/Filtered_CONTACT_Test_Cases.csv")

# # Generate responses for all questions using the RAG system
# data["model_response"] = data["Question"].apply(generate_response_rag)

# # Prepare the references and predictions
# references = data["Expected_Answer"].tolist()
# predictions = data["model_response"].tolist()

# # Compute BERTScore
# P, R, F1 = score(predictions, references, lang="en")

# # Add scores to the DataFrame
# data["Precision"] = P.tolist()
# data["Recall"] = R.tolist()
# data["F1"] = F1.tolist()

# # Display average scores
# print(f"Average Precision: {P.mean():.4f}")
# print(f"Average Recall: {R.mean():.4f}")
# print(f"Average F1 Score: {F1.mean():.4f}")

# # Save the results to a CSV file
# data.to_csv("rag_evaluation_results_STF.csv", index=False)
# print("Results saved to rag_evaluation_results.csv")

# import pandas as pd
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from bert_score import score
# import chromadb
# import os
# from dotenv import load_dotenv

# # Path to your persistent storage for ChromaDB
# load_dotenv('/Users/shoumikdaterao/Desktop/Sem\\ 7/Project/code/code/.env.local')
# storage_path = os.getenv('STORAGE_PATH')
# if storage_path is None:
#     raise ValueError('STORAGE_PATH environment variable is not set')

# # Initialize ChromaDB client and collection
# client = chromadb.PersistentClient(path=storage_path)
# collection = client.get_collection(name="customer_support_chatbot_data")

# # Load the tokenizer and model
# tokenizer = AutoTokenizer.from_pretrained("khalednabawi11/fine_tuned_dialo-gpt")
# model = AutoModelForCausalLM.from_pretrained("khalednabawi11/fine_tuned_dialo-gpt")

# # Retrieve relevant context from ChromaDB
# def retrieve_context(question, num_results=3):
#     # Embed the query using a Sentence Transformer
#     from sentence_transformers import SentenceTransformer
#     embedder = SentenceTransformer('all-MiniLM-L6-v2')
#     query_embedding = embedder.encode(question, convert_to_tensor=True).cpu().numpy()
    
#     # Query the ChromaDB collection
#     results = collection.query(query_embeddings=[query_embedding], n_results=num_results)
    
#     # Format the retrieved documents
#     if "documents" in results and results["documents"]:
#         context = "\n\n".join(doc for doc in results["documents"][0])
#         return context
#     else:
#         return "No relevant context found."

# # Function to generate model responses using RAG
# def generate_response_rag(question):
#     # Retrieve relevant context
#     context = retrieve_context(question)
    
#     # Combine context and question
#     input_text = f"{context} {tokenizer.eos_token} {question}"
#     input_tokens = tokenizer.encode(input_text, truncation=True, max_length=model.config.n_positions - 50)

#     if len(input_tokens) >= model.config.n_positions:
#         # Truncate context if necessary
#         context = context[: len(context) // 2]  # Use half the context
#         input_text = f"{context} {tokenizer.eos_token} {question}"
    
#     inputs = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=model.config.n_positions)

#     outputs = model.generate(
#         inputs,
#         max_new_tokens = 256, # Ensure max_length <= n_positions
#         pad_token_id=tokenizer.eos_token_id,
#         temperature=0.7,
#         do_sample=True,
#         top_p=0.9,
#     )
#     return tokenizer.decode(outputs[0], skip_special_tokens=True)



# # Load the CSV file (ensure it has columns: 'Question' and 'Expected Answer')
# data = pd.read_csv(r"/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Filtered test cases/Filtered_CANCEL_Test_Cases.csv")

# # Generate responses for all questions using the RAG system
# data["model_response"] = data["Question"].apply(generate_response_rag)

# # Prepare the references and predictions
# references = data["Expected_Answer"].tolist()
# predictions = data["model_response"].tolist()

# # Compute BERTScore
# P, R, F1 = score(predictions, references, lang="en")

# # Add scores to the DataFrame
# data["Precision"] = P.tolist()
# data["Recall"] = R.tolist()
# data["F1"] = F1.tolist()

# # Display average scores
# print(f"Average Precision: {P.mean():.4f}")
# print(f"Average Recall: {R.mean():.4f}")
# print(f"Average F1 Score: {F1.mean():.4f}")

# # Save the results to a CSV file
# data.to_csv("/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Backend/STF/rag_evaluation_results_CANCEL.csv", index=False)
# print("Results saved to rag_evaluation_results_ACCOUNT.csv")

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from bert_score import score
import chromadb
import os
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# Step 1: Load environment variables
load_dotenv('/Users/shoumikdaterao/Desktop/Sem\\ 7/Project/code/code/.env.local')
storage_path = os.getenv('STORAGE_PATH')
if storage_path is None:
    raise ValueError('STORAGE_PATH environment variable is not set')

# Initialize ChromaDB client and collection
client = chromadb.PersistentClient(path=storage_path)
collection = client.get_collection(name="customer_support_chatbot_data")

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("khalednabawi11/fine_tuned_dialo-gpt")
model = AutoModelForCausalLM.from_pretrained("khalednabawi11/fine_tuned_dialo-gpt")

# Sentence Transformer for query embeddings
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Retrieve relevant context from ChromaDB
def retrieve_context(question, num_results=3):
    try:
        query_embedding = embedder.encode(question, convert_to_tensor=True).cpu().numpy()
        results = collection.query(query_embeddings=[query_embedding], n_results=num_results)
        
        # Extract and format documents
        documents = results.get("documents", [[]])[0]
        if documents:
            return "\n\n".join(doc for doc in documents)
        else:
            return "No relevant context found."
    except Exception as e:
        return f"Error retrieving context: {e}"

# Function to generate responses using RAG
def generate_response(question):
    # Encode the question for input
    context = retrieve_context(question)
    
    input_text = f"Context: {context}\n\nQuestion: {question}"
    
    inputs = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors="pt")
    outputs = model.generate(
        inputs,
        attention_mask = inputs['attention_mask'],
        max_new_tokens=100,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1,
        temperature=0.7, 
        do_sample = True,
        top_p=0.9        # Use nucleus sampling
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Load the CSV file with test cases
csv_file_path = r"/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Backend/STF/rag_evaluation_results_CANCEL.csv"
data = pd.read_csv(csv_file_path)

# Generate responses for all questions
# data["model_response"] = data["Question"].apply(generate_response)
data["model_response"] = data["Question"].apply(lambda q: generate_response(q) if pd.notna(q) else "No question provided.")

# Prepare references and predictions
references = data["Expected_Answer"].fillna("").tolist()
predictions = data["model_response"].fillna("").tolist()

# Compute BERTScore
try:
    P, R, F1 = score(predictions, references, lang="en")

    # Add scores to the DataFrame
    data["Precision"] = P.tolist()
    data["Recall"] = R.tolist()
    data["F1"] = F1.tolist()

    # Display average scores
    print(f"Average Precision: {P.mean():.4f}")
    print(f"Average Recall: {R.mean():.4f}")
    print(f"Average F1 Score: {F1.mean():.4f}")
except Exception as e:
    print(f"Error computing BERTScore: {e}")

# Save the results to a CSV file
output_file_path = r"/Users/shoumikdaterao/Desktop/Sem 7/Project/code/Customer Support/Backend/STF/rag_evaluation_results_CANCEL_part2.csv"
data.to_csv(output_file_path, index=False)
print(f"Results saved to {output_file_path}")
